---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r}
library(tidyverse)
library(stringr)
```

```{r}
goodreads_data <- 
  read_csv("data/books.csv",
           locale = locale(date_format = "%m/%d/%Y")
           )

```

Trying to read in publication date as date. Doesn't work. I guess the format isn't working for as.Date
```{r}
# test_goodreads <- read_csv("data/books.csv", col_types = cols('publication_date' = col_date()))
```

# Getting to know data without code

This dataset contains observations on 8472 books gathered by the website GoodReads, which allows users to rate and search books. It was collected and cleaned by user Jealous Leopard on Kaggle and scraped directly from GoodReads.

From a quick glance, the data seems to contain one unique entry for each novel, rather than for every issue of the book. This dataset contains information about the book 
   - title, author(s), ISBN and ISBN13, language, number of pages, publication date and publisher - 
and information unique to the GoodReads site   
    - the bookID, rating, number of ratings, number of text ratings - 
    
We do not have data for any book published after 2010, which suggests that the dataset is not up to date.

The dataset does not include a genre field, which precludes any analysis based on genre unless we combine our dataset with a reliable directory containing our books and categorised into genre. This may be available from an online bookseller or a library.

Language code is provided in a coded character format. Some of the codes are easily guessable. At first glance, all of the codes are 3 letter strings which suggests these languages are coded in ISO-639-2 or ISN-639-3.

According to Wikipedia, ISBN-13 is a replacement for the original 10 digit ISBN codes. 10 digit ISBN codes can be converted to ISBN13 by appending 978 so we should be able to work with the data without the ISBN code.

When loading in the GoodReads data, RStudio had problems parsing 2 rows (1570 and 3349). The unreadable values have been replaced with NAs. This is not significant in a dataset of 8472 observations.

All books with an average rating of 0.0 have 0 ratings, so we will drop these.
A number of books with a 0 rating count have average ratings of up to 5.0. We will check the spread of ratings and drop those with very small numbers of ratings. As this is a 5 star rating system, I will definitely remove any entries with less than 5 raters.

Some books are listed as 0 pages, which seems innaccurate so I will convert these to NA.

One book has a publication date of 1 which I will also assume to be an NA value.

The data in rows 8471 and 8470 seems to have been moved along by one column. This is likely to be a manual error due to something like an extra comma, I will drop this row as I cannot be sure that the values will match up.

# Dataset cleaning for my convenience and preferences

Drop ISBN and use dim to check we have dropped one column

```{r}
dim(goodreads_data)
# 12 columns

goodreads_data <- (select(goodreads_data, -isbn))

dim(goodreads_data)
# 11 columns
```

Drop row if number of ratings is less than 5
```{r}
goodreads_data <- 
  goodreads_data %>% 
  filter(ratings_count > 5)
# Leaves 8153 observations
```

Convert 0 number of pages to NA
```{r}
# Failed attempt ------------------------------------------------------------

# goodreads_data <-
#  goodreads_data %>% 
#    mutate(
#      num_pages = ifelse(num_pages ==0, NA, num_pages)
#    )

# Success -------------------------------------------------------------------

goodreads_data <- goodreads_data %>% 
  mutate(
    num_pages = na_if(num_pages, 0)
  )
```

Convert dates of 0 to NA
```{r}
goodreads_data <- goodreads_data %>% 
  mutate(
    publication_date = na_if(publication_date, 0)
  )
```

Convert publication date to numeric
```{r}
# Failed attempt -----------------------------------------------------

#test_goodreads <- goodreads_data %>% 
#  mutate(
#    publication_date = as.Date(publication_date, "m/d/%y")
#  )

# Success ------------------------------------------------------------------

test_goodreads <-
  read_csv("data/books.csv",
           locale = locale(date_format = "%m/%d/%Y")
           )
```

drop row 8471 and 8470 "Brown's Star Atlas: Showing All The Bright Stars With Full Instructions How To Find And Use Them For Navigational Purposes And Department Of Trade Examinations." "Streetcar Suburbs"

```{r}

# Test -------------------------------------------------------------------
#test_goodreads <- goodreads_data %>% 
#  filter(
#    !is.na(average_rating)
#  )
# Removes 2 observations as expected (from 8153 to 8151)

# Implement -------------------------------------------------------------------

goodreads_data <- goodreads_data %>% 
  filter(
    !is.na(average_rating)
  )
```

# Questions to ask of the dataset
Note that all of these observations exlcude books with less than 5 ratings or with a missing average rating. This enables us to more accurately and easily explore popularity.

## Interesting facts
- most reviewed

The top 3 most rated books are The Hobbit, The Catcher in the Rye and Angels & Demons. We can surmise that these are the books most often read by GoodReads userbase (at least by those who record and rate their reading on the site).

4 of the 7 Harry Potter books appear in the top 10 most rated books.

Interestingly book 1, Harry Potter and the Philosophers Stone, does not appear in the top 10 most rated books. 
However Harry Potter books have different titles in the UK and US so perhaps there are two records for book 1.

Checking number of records for each Harry Potter book. Only book 6 "Harry Potter and the Half-Blood Prince" appears twice.

Combining the number of ratings for the 2 HP Book 6 makes no difference to the top 10 most rated books :facepalm:
```{r}
# find most read books --------------------------------------------------------

goodreads_data %>% 
  arrange(desc(ratings_count)) %>% 
  select(title, average_rating, authors, ratings_count) %>% 
  head(10)
```

```{r}
# Finding all Harry Potter books ---------------------------------------------

goodreads_data %>% 
  filter(
    str_detect(title, "Harry Potter and the")
  ) %>% 
  pull(title)

# Finding the two versions of HP Book 6 - IDnumbers 1 and 2005 ----------------

goodreads_data %>% 
  filter(
    str_detect(title, "Harry Potter and the Half-Blood")
  )

# Adding ratings count from 2nd version of Half Blood to 1st version using df goodreads_test to check that our new number was higher. -----------------------------------------------------------------------------

goodreads_test <- goodreads_data

goodreads_test[1, "ratings_count"] <- goodreads_test[1, "ratings_count"] + goodreads_test[2005, "ratings_count"]

goodreads_test[1, "ratings_count"] > goodreads_data[1, "ratings_count"]

goodreads_data[1, "ratings_count"] <- goodreads_data[1, "ratings_count"] + goodreads_data[2005, "ratings_count"]
  
```

- longest book
The record with the largest number of pages is The Complete Aubrey/Maturin Novels (5 Volumes), but as this is a set of 5 volumes, I would say that "The Second World War" by Winston S. Churchill and John Keegan is the longest book in our dataset with 4736 pages!

```{r}
goodreads_data %>% 
  arrange(desc(num_pages)) %>% 
  select(title, authors, num_pages)
  head(5)
```

- highest rated book
The most highly rated book is "Existential Meditation" by Cleveland. Interestingly. I would categorise 3 out of the 5 most highly rated books as philosophical - "Existential Meditation", "Little Book for God's Children" and "The Complete Calvin and Hobbes".
I would definitely be interested in exploring this dataset with an additional genre category.

```{r}
goodreads_data %>% 
  arrange(desc(average_rating)) %>% 
  select(title, authors, average_rating) %>% 
  head(5)
```


## Useful info for analysis
- average number of reviews
The books in this dataset have 20193.38 reviews on average.
The lowest number of reviews is 6 as I removed all books with 5 reviews or less.
The highest number of reviews is 2530894.

```{r}
goodreads_data %>%
  mutate(average_num_reviews = mean(ratings_count)) %>% 
  head(1) %>% 
  pull(average_num_reviews)

goodreads_data %>% 
  arrange(desc(ratings_count)) %>% 
  head(1) %>% 
  pull(ratings_count)
```

- average rating
The books in this dataset have an average rating of 3.95 and I have stored this value in a value called average_rating_all.
I think there is definitely a simpler way to code this!
```{r}
average_rating_all <- goodreads_data %>%
  mutate(average_rating_all = mean(average_rating)) %>% 
  head(1) %>% 
  pull(average_rating_all)
```

- number of languages (and explore the codes for English)

We have 20 distinct language codes in this dataset.
It appears that we have multiple codes for English - en-CA, en-GB, en-US and eng, enm (Middle English)
The most read language on GoodReads is English.
Spanish, Swedish, French have between 2500 and 6900 ratings. All other languages in the dataset have less than 1000 average ratings per book.
```{r}
goodreads_data %>% 
  group_by(language_code) %>% 
  summarise(ratings_count = mean(ratings_count)) %>% 
  arrange(desc(ratings_count))
```


## Questions we can answer
- How does release date impact popularity?

We have the highest number of books included for the years 2002-2006 with over 500 books published in that year included in the dataset. This would be easier to explore in a graph.

The 5 years with the highest average rating for all books fall between 1919 and 1961, but many of these years are only including one book so this is not a significant finding.
When we drop any years which have less than 10 books included in the dataset we find that the top 5 most highly rated years fall between 1990 and 2013.

Our top 5 most rated years for published books fall between 1980 and 2013.

I would conclude that the users from this dataset have more interest and higher ratings for books from the late 20th C and early 21st C.

```{r}
# How many books per year? -------------------------------------------------------

goodreads_data %>% 
  select(title, average_rating, ratings_count, publication_date) %>% 
  mutate(pub_year = format(publication_date, "%Y")) %>% 
  group_by(pub_year) %>% 
  summarise(
    avg_rating_per_year = mean(average_rating),
    books_count = n()
  ) %>% 
  arrange(desc(books_count)) %>% 
  head(10)
```


```{r}
# Finding the average rating for each year of publication excluding years with less than 10 books published ------------------------------------------------------

goodreads_data %>% 
  select(title, average_rating, ratings_count, publication_date) %>% 
  mutate(pub_year = format(publication_date, "%Y")) %>% 
  group_by(pub_year) %>% 
  summarise(
    avg_rating_per_year = mean(average_rating),
    books_count = n()
  ) %>% 
  filter(books_count > 10) %>% 
  arrange(desc(avg_rating_per_year)) %>% 
  head(10)

# Finding the average number of ratings for each year of publication excluding years with less than 10 books published -------------------------------------

goodreads_data %>% 
  select(title, average_rating, ratings_count, publication_date) %>% 
  mutate(pub_year = format(publication_date, "%Y")) %>% 
  group_by(pub_year) %>% 
  summarise(
    avg_count_per_year = mean(ratings_count),
    books_count = n()
  ) %>% 
  filter(books_count > 10) %>% 
  arrange(desc(avg_count_per_year)) %>% 
  head(10)
```

I could explore this question further and better by:
calculate coefficient between release date and average rating
group into sensible categories by year and calculate average rating
make a bar chart showing average rating by year of publication


### How does the length of the book relate to how much readers like it?

I used information from https://www.standoutbooks.com/how-long-book/ to categorise the type of book by its length into short story/children's book, novella, novel and epic lengths based on their number of pages.

Readers on GoodReads rate our longest and shortest categories - epic novels and childrens book/short story slightly higher than books of novella and novel length.

They are more likely to have reviewed a book if it is epic or novel length. Perhaps these books are more memorable, or more worth sharing with your reading community than books which take less time to read?
```{r}
goodreads_data %>% 
  mutate(length_type = 
           case_when(
             num_pages < 40 ~ "Children or Short Story",
             num_pages < 160 ~ "Novella",
             num_pages < 440 ~ "Novel",
             num_pages > 440 ~ "Epic"
           )
  ) %>% 
  group_by(length_type) %>% 
  summarise(
    rating = mean(average_rating),
    read_count = mean(ratings_count),
    count = n()
  ) %>% 
  arrange(desc(read_count))
```

### Are shorter books read more often?

categorise books by length (use a standard metric like novella, etc.) and then calculate ratings count.

Our analysis shows that GoodReads users are less likely to have rated a book of shorter length types than longer books, but this does not really tell us whether these users are more or less likely to read a book depending on its length.

### Are books with longer titles more boring?

compare length of string (perhaps tokenised into words) in title with average rating

The book with the longest title in this dataset has 212,268 characters in it. Annoyingly this is because this book title actually contains the values for a load of books which didn't parse correctly.
This is a problem I no longer have time to solve :(

The longest real title has 254 characters and the shortest has 2 "V.".
We can roughly categorise title lengths as short < 40, medium 40-80 and long > 80.
With these categories, there is very little change in rating according to length of title, but read count definitely descends with the length of the books title.


```{r}
# Finding spread of title length ---------------------------------
goodreads_data %>% 
  select(title, average_rating, ratings_count) %>% 
  mutate(
    length_title = nchar(title)
  ) %>% 
  arrange(desc(length_title)) %>% 
  filter(length_title < 1000) %>% 
  tail(1)

# Categorising by length and looking at stats ----------------------------------

goodreads_data %>% 
  select(title, average_rating, ratings_count) %>% 
  mutate(
    length_title = nchar(title)
  ) %>% 
  mutate(length_title = 
           case_when(
            length_title < 40 ~ "short_title",
            length_title < 80 ~ "medium_title",
            TRUE ~ "long_title"
           )
  ) %>% 
  group_by(length_title) %>% 
  summarise(
    rating = mean(average_rating),
    read_count = mean(ratings_count),
    count = n()
  ) %>% 
  arrange(desc(read_count)) 
```


### Can we guess what language(s) GoodReads users speak?

Some language analysis above, there are clearly more popular languages in the dataset which can be grouped intuitively - English is overwhelmingly the most common, then 3 languages with middling numbers and the rest tailing off.
We could also categorise languages as European or not European, romance or not, etc. and compare the popularity of the categories.

### Explore HowTo books as a category
use str_detect("how to") or grep to find all the how to books and compare them against the dataset as a whole.

### Explore volumes as a category
as above with "#"